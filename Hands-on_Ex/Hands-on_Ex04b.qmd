---
title: "Hands-on Exercise 4b: Visual Statistical Analysis"
author: "Sindy"
date-modified: "last-modified"
execute:
  echo: true
  eval: true
  warning: false
  freeze: true
---

## 1. Getting started

### 1.1 Visual Statistical Analysis with ggstatsplot

`ggstatsplot` is an extension of `ggplot2` package for creating graphics with details from statistical tests included in the information-rich plots themselves.

![](images/image2%20(2).jpg)

### 1.2 Installing and launching R packages

```{r}
pacman::p_load(ggstatsplot, tidyverse)
```

### 1.3 Importing data

```{r}
exam_data <- read_csv("data/Exam_data.csv")
```

### 1.4 One-sample test: gghistostats() method

In the code chunk below, `gghistostats()` is used to to build an visual of one-sample test on English scores.

```{r}
set.seed(1234)

gghistostats(
  data = exam_data,
  x = ENGLISH,
  type = "bayes",
  test.value = 60,
  xlab = "English scores"
)
```

::: panel-tabset
### Code breakdown

`gghistostats()`: This function creates a histogram with statistical annotations.

-   `x = ENGLISH`: Specifies the variable (English scores) to be analyzed.
-   `type = "bayes"`: Indicates a Bayesian one-sample test is conducted.
-   `test.value = 60`: The test value for comparison, meaning the function tests whether the mean English score significantly differs from 60.
-   `xlab = "English scores"`: Labels the x-axis as "English scores."

### Explanation of output

-   Histogram: Displays the distribution of English scores with gray bars.
-   Y-axis (left: count, right: proportion): Shows the frequency and proportion of students scoring within certain ranges.
-   Dashed Blue Line: Represents the estimated mean (Maximum A Posteriori estimate, $\hat{\mu}_{MAP}$), which is approximately 74.74.
-   Statistical Annotations:
    -   $\log_e(BF_{01}) = -31.45$: The natural log of the Bayes factor, indicating very strong evidence against the null hypothesis (which assumes a mean of 60).
    -   $\delta_{\text{difference}}^{\text{posterior}} = 7.16$: The estimated mean difference between the sample mean and 60.
    -   $CI^{ETI}_{95\%} [5.54, 8.75]$: The 95% credible interval (Highest Density Interval) for the mean difference.
    -   $r^{JZS}_{Cauchy} = 0.71$: The effect size based on the Jeffreys–Zellner–Siow (JZS) prior.

### Key Interpretation

-   The English scores are right-skewed and centered around **74.74**, which is significantly higher than the test value of **60**.
-   The **negative log Bayes factor (-31.45)** provides overwhelming evidence against the null hypothesis.
-   The **credible interval \[5.54, 8.75\]** indicates that the true mean difference is highly likely within this range, showing strong evidence that the students’ average English scores are significantly above **60**.
:::

## 2. Bayes Factor

### 2.1 Unpacking the Bayes Factor

-   A **Bayes Factor (BF)** is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.

-   The Bayes Factor provides a way to evaluate data in favor of a null hypothesis and to incorporate external information in doing so. It quantifies the **weight of the evidence** in favor of a given hypothesis.

-   When comparing two hypotheses, $H_1$ (the alternative hypothesis) and $H_0$ (the null hypothesis), the Bayes Factor is often written as $BF_{10}$. Mathematically, it is defined as:

    $$
    BF_{10} = \frac{P(D \mid H_1)}{P(D \mid H_0)}
    $$

where:

-   $P(D \mid H_1)$ is the probability of the observed data given that the alternative hypothesis is true.

-   $P(D \mid H_0)$ is the probability of the observed data given that the null hypothesis is true.

-   A **Bayes Factor greater than 1** indicates evidence in favor of $H_1$, while a **Bayes Factor less than 1** supports $H_0$.

The [**Schwarz criterion**](https://www.statisticshowto.com/bayesian-information-criterion/) (Bayesian Information Criterion, **BIC**) is one of the simplest ways to approximate the Bayes Factor.

### 2.2 How to interpret Bayes Factor

A **Bayes Factor** can be any positive number. One of the most common interpretations is this one---first proposed by Harold Jeffereys (1961) and slightly modified by [Lee and Wagenmakers](https://www-tandfonline-com.libproxy.smu.edu.sg/doi/pdf/10.1080/00031305.1999.10474443?needAccess=true) in 2013:

![](images/image6.jpg){width="480"}

## 3. Hypothesis Testing

### 3.1 Two-sample mean test: *ggbetweenstats()*

In the code chunk below, [*ggbetweenstats()*](https://indrajeetpatil.github.io/ggstatsplot/reference/ggbetweenstats.html) is used to build a visual for two-sample mean test of Maths scores by gender.

```{r}
ggbetweenstats(
  data = exam_data,
  x = GENDER, 
  y = MATHS,
  type = "np",
  messages = FALSE
)
```

::: panel-tabset
### Code breakdown

-   `x = GENDER` → Categorical variable (independent variable) representing gender groups (Male & Female).
-   `y = MATHS` → Numeric variable (dependent variable) representing Maths scores.
-   `type = "np"` → Specifies a nonparametric test (Mann-Whitney U test, also known as the Wilcoxon rank-sum test) instead of a parametric t-test.

### Explanation of output

1.  **Violin Plots**:
    -   Show the distribution of Maths scores for **Female (left, teal)** and **Male (right, orange)**.
    -   The width of the violin represents the density of data points.
2.  **Boxplots Inside Violin Plots**:
    -   The black box within each violin represents the **interquartile range (IQR)** (middle 50% of data).
    -   The black horizontal line inside the box represents the **median**.
    -   The whiskers extend to the smallest and largest values within 1.5 times the IQR.
3.  **Individual Data Points**:
    -   Each dot represents an individual student's Maths score.
    -   Provides insight into the spread and density of scores.
4.  **Mann-Whitney U Test Results (Top Annotation)**:
    -   $W_{Mann-Whitney} = 13011.00$ → The Mann-Whitney U test statistic.
    -   $p = 0.91$ → High p-value suggests no significant difference between the two groups.
    -   $\hat{r}_{biserial}^{rank} = 7.04e-03$ → Rank-biserial correlation effect size (very small effect).
    -   $CI_{95\%} [-0.12, 0.13]$ → 95% confidence interval for the effect size.
    -   $n_{obs} = 322$ → Total number of observations (170 females, 152 males).

### Key Interpretation

-   The **p-value (0.91) is very high**, suggesting **no statistically significant difference** in Maths scores between genders.
-   The **confidence interval \[-0.12, 0.13\] includes zero**, reinforcing the lack of a meaningful effect.
-   The **effect size is nearly zero**, further indicating no meaningful difference in Maths performance based on gender.
:::

### 3.2 Oneway ANOVA Test: ggbetweenstats() method

In the code chunk below, `ggbetweenstats()` is used to build a visual for One-way ANOVA test on English score by race.

```{r}
ggbetweenstats(
  data = exam_data,
  x = RACE, 
  y = ENGLISH,
  type = "p",
  mean.ci = TRUE, 
  pairwise.comparisons = TRUE, 
  pairwise.display = "s",
  p.adjust.method = "fdr",
  messages = FALSE
)
```

::: panel-tabset
### Code breakdown

-   `type = "p"` → Performs a parametric test (Welch’s ANOVA for unequal variances).

-   `mean.ci = TRUE` → Displays mean and confidence intervals for each group.

-   `pairwise.comparisons = TRUE` → Conducts post-hoc pairwise comparisons (e.g., Games-Howell test for unequal variances).

-   `pairwise.display = "s"`

    -   `"ns"` → Shows only non-significant comparisons.

    -   `"s"` → Shows only significant comparisons (used in this case).

    -   `"all"` → Shows all comparisons.

-   `p.adjust.method = "fdr"` → Adjusts p-values for multiple comparisons using the False Discovery Rate (FDR) correction.

-   `messages = FALSE` → Suppresses console messages.

### Explanation of output

#### 1. One-Way Welch ANOVA Results

-   $F_{\text{Welch}}(3, 23.8) = 10.15$ → The Welch’s ANOVA test statistic\
-   $p = 1.71 \times 10^{-4}$ → The p-value, indicating a statistically significant difference among groups\
-   $\hat{\omega}^2_p = 0.50$ → Effect size (moderate to large effect)\
-   $CI_{95\%} [0.21, 1.00]$ → Confidence interval for the effect size\
-   $n_{\text{obs}} = 322$ → Number of observations

This suggests that there is a significant difference in English scores across racial groups.

#### 2. Violin & Boxplots

-   The violin plot shows the distribution of scores.\
-   The boxplot (inside the violin plot) summarizes:
    -   The median (middle line in the box)
    -   The interquartile range (box)
    -   The whiskers (spread of data)
    -   The mean (red dot with label)

#### 3. Post-Hoc Pairwise Comparisons

-   The **Games-Howell test** is used for post-hoc analysis (adjusted for multiple comparisons).\
-   Only significant comparisons are displayed.\
-   The p-value adjustment method used is **False Discovery Rate (FDR)**.

#### 4. Bayesian Statistics

At the bottom:

-   $\log_e(BF_{01}) = -11.63$ → Bayesian evidence against the null hypothesis\
-   $R^2_{\text{posterior}} = 0.09$ → Bayesian effect size\
-   $CI_{95\%} [0.04, 0.15]$ → Bayesian credible interval\
-   $r_{\text{Cauchy}} = 0.71$ → Cauchy prior width used

The negative log Bayes Factor $BF_{01}$ suggests strong evidence **against the null hypothesis**, meaning there is a significant difference between the groups.

### Key Interpretation

-   The one-way Welch ANOVA confirms a significant difference in English scores across racial groups.
-   The Chinese and "Other" groups have higher mean scores than the Indian and Malay groups.
-   Post-hoc Games-Howell comparisons highlight significant differences.
-   Bayesian analysis supports the findings from the frequentist approach.
:::

![](images/image9.jpg)

### 3.3 Significance Test of Correlation: ggscatterstats()

In the code chunk below, `ggscatterstats()` is used to build a visualization for the Significance Test of Correlation between Maths scores and English scores.

```{r}
ggscatterstats(
  data = exam_data,
  x = MATHS,
  y = ENGLISH,
  marginal = FALSE,
  )
```

### 3.4 Significance Test of Association (Dependence): ggbarstats() Method

In the code chunk below, the Maths scores are binned into a 4-class variable using the `cut()` function.

```{r}
exam1 <- exam_data %>% 
  mutate(MATHS_bins = 
           cut(MATHS, 
               breaks = c(0, 60, 75, 85, 100))
  )

```

```{r}
ggbarstats(
  data = exam1, 
  x = MATHS_bins, 
  y = GENDER
)
```

## 4. Visualising Models

In this section, Toyota Corolla case study will be used. The purpose of study is to build a model to discover factors affecting prices of used-cars by taking into consideration a set of explanatory variables.

### 4.1 Visualising Models

```{r}
pacman::p_load(readxl, performance, parameters, see)
```

### 4.2 Importing Excel File: readxl Methods

```{r}
car_resale <- read_xls("data/ToyotaCorolla.xls", 
                       "data")
car_resale
```

Note that the output object car_resale is a tibble data frame.

## 5. Building and Diagnosing a Multiple Regression Model

### 5.1 Multiple Regression Model using lm()

The code chunk below is used to calibrate a multiple linear regression model by using `lm()` of Base Stats of R.

```{r}
model <- lm(Price ~ Age_08_04 + Mfg_Year + KM + 
              Weight + Guarantee_Period, data = car_resale)
model
```

### 5.2 Multiple Regression Model using lm()

```{r}
check_collinearity(model)
```

```{r}
check_c <- check_collinearity(model)
plot(check_c)
```

### 5.3 Model Diagnostic: checking normality assumption

```{r}
model1 <- lm(Price ~ Age_08_04 + KM + 
              Weight + Guarantee_Period, data = car_resale)
```

```{r}
check_n <- check_normality(model1)
plot(check_n)
```

### 5.4 Model Diagnostic: Check model for homogeneity of variances

```{r}
check_h <- check_heteroscedasticity(model1)
plot(check_h)
```

### 5.5 Model Diagnostic: Complete check

```{r fig.width=12, fig.height=10}
check_model(model1)
```

## 6. Visualizing Regression Results

### 6.1 Visualising Regression Parameters: see methods

In the code below, `plot()` of see package and `parameters()` of parameters package is used to visualise the parameters of a regression model.

```{r}
plot(parameters(model1))
```

### 6.2 Visualising Regression Parameters: ggcoefstats() methods

In the code below, `ggcoefstats()` of ggstatsplot package to visualise the parameters of a regression model.

```{r}
ggcoefstats(model1, 
            output = "plot")
```

## 7. Reference

Credits to [Prof Kam](https://r4va.netlify.app/chap10).

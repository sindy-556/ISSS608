---
title: "Hands-on Exercise 4b: Visual Statistical Analysis"
author: "Sindy"
date-modified: "last-modified"
execute:
  echo: true
  eval: true
  warning: false
  freeze: true
---

## 1. Getting started

### 1.1 Visual Statistical Analysis with ggstatsplot

`ggstatsplot` is an extension of `ggplot2` package for creating graphics with details from statistical tests included in the information-rich plots themselves.

![](images/image2%20(2).jpg)

### 1.2 Installing and launching R packages

```{r}
pacman::p_load(ggstatsplot, tidyverse)
```

### 1.3 Importing data

```{r}
exam_data <- read_csv("data/Exam_data.csv")
```

### 1.4 One-sample test: gghistostats() method

In the code chunk below, `gghistostats()` is used to to build an visual of one-sample test on English scores.

```{r}
set.seed(1234)

gghistostats(
  data = exam_data,
  x = ENGLISH,
  type = "bayes",
  test.value = 60,
  xlab = "English scores"
)
```

::: panel-tabset
### Code breakdown

`gghistostats()`: This function creates a histogram with statistical annotations.

-   `x = ENGLISH`: Specifies the variable (English scores) to be analyzed.
-   `type = "bayes"`: Indicates a Bayesian one-sample test is conducted.
-   `test.value = 60`: The test value for comparison, meaning the function tests whether the mean English score significantly differs from 60.
-   `xlab = "English scores"`: Labels the x-axis as "English scores."

### Explanation of output

-   Histogram: Displays the distribution of English scores with gray bars.
-   Y-axis (left: count, right: proportion): Shows the frequency and proportion of students scoring within certain ranges.
-   Dashed Blue Line: Represents the estimated mean (Maximum A Posteriori estimate, $\hat{\mu}_{MAP}$), which is approximately 74.74.
-   Statistical Annotations:
    -   $\log_e(BF_{01}) = -31.45$: The natural log of the Bayes factor, indicating very strong evidence against the null hypothesis (which assumes a mean of 60).
    -   $\delta_{\text{difference}}^{\text{posterior}} = 7.16$: The estimated mean difference between the sample mean and 60.
    -   $CI^{ETI}_{95\%} [5.54, 8.75]$: The 95% credible interval (Highest Density Interval) for the mean difference.
    -   $r^{JZS}_{Cauchy} = 0.71$: The effect size based on the Jeffreys–Zellner–Siow (JZS) prior.

### Key Interpretation

-   The English scores are right-skewed and centered around **74.74**, which is significantly higher than the test value of **60**.
-   The **negative log Bayes factor (-31.45)** provides overwhelming evidence against the null hypothesis.
-   The **credible interval \[5.54, 8.75\]** indicates that the true mean difference is highly likely within this range, showing strong evidence that the students’ average English scores are significantly above **60**.
:::

## 2. Bayes Factor

### 2.1 Unpacking the Bayes Factor

-   A **Bayes Factor (BF)** is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.

-   The Bayes Factor provides a way to evaluate data in favor of a null hypothesis and to incorporate external information in doing so. It quantifies the **weight of the evidence** in favor of a given hypothesis.

-   When comparing two hypotheses, $H_1$ (the alternative hypothesis) and $H_0$ (the null hypothesis), the Bayes Factor is often written as $BF_{10}$. Mathematically, it is defined as:

    $$
    BF_{10} = \frac{P(D \mid H_1)}{P(D \mid H_0)}
    $$

where:

-   $P(D \mid H_1)$ is the probability of the observed data given that the alternative hypothesis is true.

-   $P(D \mid H_0)$ is the probability of the observed data given that the null hypothesis is true.

-   A **Bayes Factor greater than 1** indicates evidence in favor of $H_1$, while a **Bayes Factor less than 1** supports $H_0$.

The [**Schwarz criterion**](https://www.statisticshowto.com/bayesian-information-criterion/) (Bayesian Information Criterion, **BIC**) is one of the simplest ways to approximate the Bayes Factor.

### 2.2 How to interpret Bayes Factor

A **Bayes Factor** can be any positive number. One of the most common interpretations is this one---first proposed by Harold Jeffereys (1961) and slightly modified by [Lee and Wagenmakers](https://www-tandfonline-com.libproxy.smu.edu.sg/doi/pdf/10.1080/00031305.1999.10474443?needAccess=true) in 2013:

![](images/image6.jpg){width="480"}

## 3. Hypothesis Testing

### 3.1 Two-sample mean test: *ggbetweenstats()*

In the code chunk below, [*ggbetweenstats()*](https://indrajeetpatil.github.io/ggstatsplot/reference/ggbetweenstats.html) is used to build a visual for two-sample mean test of Maths scores by gender.

```{r}
ggbetweenstats(
  data = exam_data,
  x = GENDER, 
  y = MATHS,
  type = "np",
  messages = FALSE
)
```

::: panel-tabset
### Code breakdown

-   `x = GENDER` → Categorical variable (independent variable) representing gender groups (Male & Female).
-   `y = MATHS` → Numeric variable (dependent variable) representing Maths scores.
-   `type = "np"` → Specifies a nonparametric test (Mann-Whitney U test, also known as the Wilcoxon rank-sum test) instead of a parametric t-test.

### Explanation of output

1.  **Violin Plots**:
    -   Show the distribution of Maths scores for **Female (left, teal)** and **Male (right, orange)**.
    -   The width of the violin represents the density of data points.
2.  **Boxplots Inside Violin Plots**:
    -   The black box within each violin represents the **interquartile range (IQR)** (middle 50% of data).
    -   The black horizontal line inside the box represents the **median**.
    -   The whiskers extend to the smallest and largest values within 1.5 times the IQR.
3.  **Individual Data Points**:
    -   Each dot represents an individual student's Maths score.
    -   Provides insight into the spread and density of scores.
4.  **Mann-Whitney U Test Results (Top Annotation)**:
    -   $W_{Mann-Whitney} = 13011.00$ → The Mann-Whitney U test statistic.
    -   $p = 0.91$ → High p-value suggests no significant difference between the two groups.
    -   $\hat{r}_{biserial}^{rank} = 7.04e-03$ → Rank-biserial correlation effect size (very small effect).
    -   $CI_{95\%} [-0.12, 0.13]$ → 95% confidence interval for the effect size.
    -   $n_{obs} = 322$ → Total number of observations (170 females, 152 males).

### Key Interpretation

-   The **p-value (0.91) is very high**, suggesting **no statistically significant difference** in Maths scores between genders.
-   The **confidence interval \[-0.12, 0.13\] includes zero**, reinforcing the lack of a meaningful effect.
-   The **effect size is nearly zero**, further indicating no meaningful difference in Maths performance based on gender.
:::

### 3.2 Oneway ANOVA Test: ggbetweenstats() method

In the code chunk below, `ggbetweenstats()` is used to build a visual for One-way ANOVA test on English score by race.

```{r}
ggbetweenstats(
  data = exam_data,
  x = RACE, 
  y = ENGLISH,
  type = "p",
  mean.ci = TRUE, 
  pairwise.comparisons = TRUE, 
  pairwise.display = "s",
  p.adjust.method = "fdr",
  messages = FALSE
)
```

::: panel-tabset
### Code breakdown

-   type = "p" → Performs a parametric test (Welch’s ANOVA for unequal variances).

-   mean.ci = TRUE → Displays mean and confidence intervals for each group.

-   pairwise.comparisons = TRUE → Conducts post-hoc pairwise comparisons (e.g., Games-Howell test for unequal variances).

-   pairwise.display = "s" 
    -   "ns" → Shows only non-significant comparisons.

    -   "s" → Shows only significant comparisons (used in this case).

    -   "all" → Shows all comparisons.

-   p.adjust.method = "fdr" → Adjusts p-values for multiple comparisons using the False Discovery Rate (FDR) correction.

-   messages = FALSE → Suppresses console messages.

### Explanation of output



### Key Interpretation


:::

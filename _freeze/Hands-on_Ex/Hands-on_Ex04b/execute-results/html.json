{
  "hash": "71ccbb6e14faa3968b30a2d0220c645e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Hands-on Exercise 4b: Visual Statistical Analysis\"\nauthor: \"Sindy\"\ndate-modified: \"last-modified\"\nexecute:\n  echo: true\n  eval: true\n  warning: false\n  freeze: true\n---\n\n\n\n## 1. Getting started\n\n### 1.1 Visual Statistical Analysis with ggstatsplot\n\n`ggstatsplot` is an extension of `ggplot2` package for creating graphics with details from statistical tests included in the information-rich plots themselves.\n\n![](images/image2%20(2).jpg)\n\n### 1.2 Installing and launching R packages\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(ggstatsplot, tidyverse)\n```\n:::\n\n\n\n### 1.3 Importing data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexam_data <- read_csv(\"data/Exam_data.csv\")\n```\n:::\n\n\n\n### 1.4 One-sample test: gghistostats() method\n\nIn the code chunk below, `gghistostats()` is used to to build an visual of one-sample test on English scores.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\ngghistostats(\n  data = exam_data,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n```\n\n::: {.cell-output-display}\n![](Hands-on_Ex04b_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n::: panel-tabset\n### Code breakdown\n\n`gghistostats()`: This function creates a histogram with statistical annotations.\n\n-   `x = ENGLISH`: Specifies the variable (English scores) to be analyzed.\n-   `type = \"bayes\"`: Indicates a Bayesian one-sample test is conducted.\n-   `test.value = 60`: The test value for comparison, meaning the function tests whether the mean English score significantly differs from 60.\n-   `xlab = \"English scores\"`: Labels the x-axis as \"English scores.\"\n\n### Explanation of output\n\n-   Histogram: Displays the distribution of English scores with gray bars.\n-   Y-axis (left: count, right: proportion): Shows the frequency and proportion of students scoring within certain ranges.\n-   Dashed Blue Line: Represents the estimated mean (Maximum A Posteriori estimate, $\\hat{\\mu}_{MAP}$), which is approximately 74.74.\n-   Statistical Annotations:\n    -   $\\log_e(BF_{01}) = -31.45$: The natural log of the Bayes factor, indicating very strong evidence against the null hypothesis (which assumes a mean of 60).\n    -   $\\delta_{\\text{difference}}^{\\text{posterior}} = 7.16$: The estimated mean difference between the sample mean and 60.\n    -   $CI^{ETI}_{95\\%} [5.54, 8.75]$: The 95% credible interval (Highest Density Interval) for the mean difference.\n    -   $r^{JZS}_{Cauchy} = 0.71$: The effect size based on the Jeffreys–Zellner–Siow (JZS) prior.\n\n### Key Interpretation\n\n-   The English scores are right-skewed and centered around **74.74**, which is significantly higher than the test value of **60**.\n-   The **negative log Bayes factor (-31.45)** provides overwhelming evidence against the null hypothesis.\n-   The **credible interval \\[5.54, 8.75\\]** indicates that the true mean difference is highly likely within this range, showing strong evidence that the students’ average English scores are significantly above **60**.\n:::\n\n## 2. Bayes Factor\n\n### 2.1 Unpacking the Bayes Factor\n\n-   A **Bayes Factor (BF)** is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\n\n-   The Bayes Factor provides a way to evaluate data in favor of a null hypothesis and to incorporate external information in doing so. It quantifies the **weight of the evidence** in favor of a given hypothesis.\n\n-   When comparing two hypotheses, $H_1$ (the alternative hypothesis) and $H_0$ (the null hypothesis), the Bayes Factor is often written as $BF_{10}$. Mathematically, it is defined as:\n\n    $$\n    BF_{10} = \\frac{P(D \\mid H_1)}{P(D \\mid H_0)}\n    $$\n\nwhere:\n\n-   $P(D \\mid H_1)$ is the probability of the observed data given that the alternative hypothesis is true.\n\n-   $P(D \\mid H_0)$ is the probability of the observed data given that the null hypothesis is true.\n\n-   A **Bayes Factor greater than 1** indicates evidence in favor of $H_1$, while a **Bayes Factor less than 1** supports $H_0$.\n\nThe [**Schwarz criterion**](https://www.statisticshowto.com/bayesian-information-criterion/) (Bayesian Information Criterion, **BIC**) is one of the simplest ways to approximate the Bayes Factor.\n\n### 2.2 How to interpret Bayes Factor\n\nA **Bayes Factor** can be any positive number. One of the most common interpretations is this one---first proposed by Harold Jeffereys (1961) and slightly modified by [Lee and Wagenmakers](https://www-tandfonline-com.libproxy.smu.edu.sg/doi/pdf/10.1080/00031305.1999.10474443?needAccess=true) in 2013:\n\n![](images/image6.jpg){width=\"480\"}\n\n## 3. Hypothesis Testing\n\n### 3.1 Two-sample mean test: *ggbetweenstats()*\n\nIn the code chunk below, [*ggbetweenstats()*](https://indrajeetpatil.github.io/ggstatsplot/reference/ggbetweenstats.html) is used to build a visual for two-sample mean test of Maths scores by gender.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggbetweenstats(\n  data = exam_data,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n```\n\n::: {.cell-output-display}\n![](Hands-on_Ex04b_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n::: panel-tabset\n### Code breakdown\n\n-   `x = GENDER` → Categorical variable (independent variable) representing gender groups (Male & Female).\n-   `y = MATHS` → Numeric variable (dependent variable) representing Maths scores.\n-   `type = \"np\"` → Specifies a nonparametric test (Mann-Whitney U test, also known as the Wilcoxon rank-sum test) instead of a parametric t-test.\n\n### Explanation of output\n\n1.  **Violin Plots**:\n    -   Show the distribution of Maths scores for **Female (left, teal)** and **Male (right, orange)**.\n    -   The width of the violin represents the density of data points.\n2.  **Boxplots Inside Violin Plots**:\n    -   The black box within each violin represents the **interquartile range (IQR)** (middle 50% of data).\n    -   The black horizontal line inside the box represents the **median**.\n    -   The whiskers extend to the smallest and largest values within 1.5 times the IQR.\n3.  **Individual Data Points**:\n    -   Each dot represents an individual student's Maths score.\n    -   Provides insight into the spread and density of scores.\n4.  **Mann-Whitney U Test Results (Top Annotation)**:\n    -   $W_{Mann-Whitney} = 13011.00$ → The Mann-Whitney U test statistic.\n    -   $p = 0.91$ → High p-value suggests no significant difference between the two groups.\n    -   $\\hat{r}_{biserial}^{rank} = 7.04e-03$ → Rank-biserial correlation effect size (very small effect).\n    -   $CI_{95\\%} [-0.12, 0.13]$ → 95% confidence interval for the effect size.\n    -   $n_{obs} = 322$ → Total number of observations (170 females, 152 males).\n\n### Key Interpretation\n\n-   The **p-value (0.91) is very high**, suggesting **no statistically significant difference** in Maths scores between genders.\n-   The **confidence interval \\[-0.12, 0.13\\] includes zero**, reinforcing the lack of a meaningful effect.\n-   The **effect size is nearly zero**, further indicating no meaningful difference in Maths performance based on gender.\n:::\n\n### 3.2 Oneway ANOVA Test: ggbetweenstats() method\n\nIn the code chunk below, `ggbetweenstats()` is used to build a visual for One-way ANOVA test on English score by race.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggbetweenstats(\n  data = exam_data,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n```\n\n::: {.cell-output-display}\n![](Hands-on_Ex04b_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n::: panel-tabset\n### Code breakdown\n\n-   `type = \"p\"` → Performs a parametric test (Welch’s ANOVA for unequal variances).\n\n-   `mean.ci = TRUE` → Displays mean and confidence intervals for each group.\n\n-   `pairwise.comparisons = TRUE` → Conducts post-hoc pairwise comparisons (e.g., Games-Howell test for unequal variances).\n\n-   `pairwise.display = \"s\"`\n\n    -   `\"ns\"` → Shows only non-significant comparisons.\n\n    -   `\"s\"` → Shows only significant comparisons (used in this case).\n\n    -   `\"all\"` → Shows all comparisons.\n\n-   `p.adjust.method = \"fdr\"` → Adjusts p-values for multiple comparisons using the False Discovery Rate (FDR) correction.\n\n-   `messages = FALSE` → Suppresses console messages.\n\n### Explanation of output\n\n#### 1. One-Way Welch ANOVA Results\n\n-   $F_{\\text{Welch}}(3, 23.8) = 10.15$ → The Welch’s ANOVA test statistic\\\n-   $p = 1.71 \\times 10^{-4}$ → The p-value, indicating a statistically significant difference among groups\\\n-   $\\hat{\\omega}^2_p = 0.50$ → Effect size (moderate to large effect)\\\n-   $CI_{95\\%} [0.21, 1.00]$ → Confidence interval for the effect size\\\n-   $n_{\\text{obs}} = 322$ → Number of observations\n\nThis suggests that there is a significant difference in English scores across racial groups.\n\n#### 2. Violin & Boxplots\n\n-   The violin plot shows the distribution of scores.\\\n-   The boxplot (inside the violin plot) summarizes:\n    -   The median (middle line in the box)\n    -   The interquartile range (box)\n    -   The whiskers (spread of data)\n    -   The mean (red dot with label)\n\n#### 3. Post-Hoc Pairwise Comparisons\n\n-   The **Games-Howell test** is used for post-hoc analysis (adjusted for multiple comparisons).\\\n-   Only significant comparisons are displayed.\\\n-   The p-value adjustment method used is **False Discovery Rate (FDR)**.\n\n#### 4. Bayesian Statistics\n\nAt the bottom:\n\n-   $\\log_e(BF_{01}) = -11.63$ → Bayesian evidence against the null hypothesis\\\n-   $R^2_{\\text{posterior}} = 0.09$ → Bayesian effect size\\\n-   $CI_{95\\%} [0.04, 0.15]$ → Bayesian credible interval\\\n-   $r_{\\text{Cauchy}} = 0.71$ → Cauchy prior width used\n\nThe negative log Bayes Factor $BF_{01}$ suggests strong evidence **against the null hypothesis**, meaning there is a significant difference between the groups.\n\n### Key Interpretation\n\n-   The one-way Welch ANOVA confirms a significant difference in English scores across racial groups.\n-   The Chinese and \"Other\" groups have higher mean scores than the Indian and Malay groups.\n-   Post-hoc Games-Howell comparisons highlight significant differences.\n-   Bayesian analysis supports the findings from the frequentist approach.\n:::\n\n![](images/image9.jpg)\n\n### 3.3 Significance Test of Correlation: ggscatterstats()\n\nIn the code chunk below, `ggscatterstats()` is used to build a visualization for the Significance Test of Correlation between Maths scores and English scores.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggscatterstats(\n  data = exam_data,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = FALSE,\n  )\n```\n\n::: {.cell-output-display}\n![](Hands-on_Ex04b_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n### 3.4 Significance Test of Association (Dependence): ggbarstats() Method\n\nIn the code chunk below, the Maths scores are binned into a 4-class variable using the `cut()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexam1 <- exam_data %>% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0, 60, 75, 85, 100))\n  )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggbarstats(\n  data = exam1, \n  x = MATHS_bins, \n  y = GENDER\n)\n```\n\n::: {.cell-output-display}\n![](Hands-on_Ex04b_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n## 4. Visualising Models\n\nIn this section, Toyota Corolla case study will be used. The purpose of study is to build a model to discover factors affecting prices of used-cars by taking into consideration a set of explanatory variables.\n\n### 4.1 Visualising Models\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(readxl, performance, parameters, see)\n```\n:::\n\n\n\n### 4.2 Importing Excel File: readxl Methods\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar_resale <- read_xls(\"data/ToyotaCorolla.xls\", \n                       \"data\")\ncar_resale\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   <dbl> <chr>    <dbl>     <dbl>     <dbl>    <dbl>  <dbl>         <dbl>  <dbl>\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period <dbl>, HP_Bin <chr>, CC_bin <chr>,\n#   Doors <dbl>, Gears <dbl>, Cylinders <dbl>, Fuel_Type <chr>, Color <chr>,\n#   Met_Color <dbl>, Automatic <dbl>, Mfr_Guarantee <dbl>,\n#   BOVAG_Guarantee <dbl>, ABS <dbl>, Airbag_1 <dbl>, Airbag_2 <dbl>,\n#   Airco <dbl>, Automatic_airco <dbl>, Boardcomputer <dbl>, CD_Player <dbl>,\n#   Central_Lock <dbl>, Powered_Windows <dbl>, Power_Steering <dbl>, …\n```\n\n\n:::\n:::\n\n\n\nNote that the output object car_resale is a tibble data frame.\n\n## 5. Building and Diagnosing a Multiple Regression Model\n\n### 5.1 Multiple Regression Model using lm()\n\nThe code chunk below is used to calibrate a multiple linear regression model by using `lm()` of Base Stats of R.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n```\n\n\n:::\n:::\n\n\n\n### 5.2 Multiple Regression Model using lm()\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_collinearity(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_c <- check_collinearity(model)\nplot(check_c)\n```\n\n::: {.cell-output-display}\n![](Hands-on_Ex04b_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\n### 5.3 Model Diagnostic: checking normality assumption\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel1 <- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_n <- check_normality(model1)\nplot(check_n)\n```\n\n::: {.cell-output-display}\n![](Hands-on_Ex04b_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n### 5.4 Model Diagnostic: Check model for homogeneity of variances\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_h <- check_heteroscedasticity(model1)\nplot(check_h)\n```\n\n::: {.cell-output-display}\n![](Hands-on_Ex04b_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n### 5.5 Model Diagnostic: Complete check\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_model(model1)\n```\n\n::: {.cell-output-display}\n![](Hands-on_Ex04b_files/figure-html/unnamed-chunk-17-1.png){width=1152}\n:::\n:::\n\n\n\n## 6. Visualizing Regression Results\n\n### 6.1 Visualising Regression Parameters: see methods\n\nIn the code below, `plot()` of see package and `parameters()` of parameters package is used to visualise the parameters of a regression model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(parameters(model1))\n```\n\n::: {.cell-output-display}\n![](Hands-on_Ex04b_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n### 6.2 Visualising Regression Parameters: ggcoefstats() methods\n\nIn the code below, `ggcoefstats()` of ggstatsplot package to visualise the parameters of a regression model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggcoefstats(model1, \n            output = \"plot\")\n```\n\n::: {.cell-output-display}\n![](Hands-on_Ex04b_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\n## 7. Reference\n\nCredits to [Prof Kam](https://r4va.netlify.app/chap10).\n",
    "supporting": [
      "Hands-on_Ex04b_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
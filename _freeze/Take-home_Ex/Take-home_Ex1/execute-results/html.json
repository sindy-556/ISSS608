{
  "hash": "d1591cf78d6d2540864d66b7cba0a110",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Take-home Exercise 1: Examining Heart Attack Risk in Japan\"\nauthor: \"Sindy\"\ndate-modified: \"last-modified\"\noutput:\n  html_document:\n    css: styles.css\nexecute:\n  echo: true\n  eval: true\n  warning: false\n  freeze: true\n---\n\n\n\n## Overview\n\nThis dataset investigates the epidemiology of heart attacks among different segments of the Japanese population. Japan’s rapidly aging demographic and high healthcare standards make it a unique context in which lifestyle, clinical parameters, and heart attack occurrence interact in complex ways.\n\n### Our task\n\nIn this exercise we will be: \n\n- Examining Heart Attack Occurrence: Analyze the distribution and determinants of heart attack events across the dataset. \n- Conducting Demographic Analysis: Investigate how age, gender, and region contribute to heart attack risk, distinguishing between younger and older cohorts.\n- Exploring Health Metrics: Visualize relationships between clinical indicators (e.g., BMI, blood pressure, cholesterol) and heart attack occurrence. \n- Assessing Lifestyle Factors: Evaluate the impact of lifestyle variables such as smoking history, physical activity, diet quality, alcohol consumption, and stress levels on heart health.\n\n## Getting started\n\n### Load packages\n\n\nWe load the following R packages using the `pacman::p_load()` function:\n\n- **tidyverse**: Core collection of R packages for data wrangling and visualization (e.g., `dplyr`, `ggplot2`)  \n- **SmartEDA**: For the `ExpData()` function used in exploratory data analysis  \n- **easystats**: Specifically for `check_collinearity()` to diagnose multicollinearity issues  \n- **reshape2**: Provides the `melt()` function for reshaping data from wide to long format  \n- **caret**: Functions for data partitioning (`createDataPartition`) and model training workflows  \n- **yardstick**: Offers `conf_mat()` and other classification metrics  \n- **pROC**: For ROC curves and AUC calculations (`roc`, `auc`)  \n- **GGally**: For the `ggpairs()` function to create pairwise scatterplot matrices  \n- **ggmosaic**: To create mosaic plots via `geom_mosaic()`  \n- **patchwork**: For arranging multiple ggplot figures into a composite layout  \n- **xgboost**: Gradient boosting library for classification and regression tasks\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(tidyverse, SmartEDA, easystats, reshape2, caret, yardstick, pROC, GGally, ggmosaic, patchwork, xgboost)\n```\n:::\n\n\n\n\nThis dataset contains information about heart attack occurrences in Japan, focusing on various demographic and health-related factors.\n\n### Import data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nheart_data <- read_csv(\"./data/japan_heart_attack_dataset.csv\")\n```\n:::\n\n\n\n## Data pre-processing\n\n### Glimpse of data\nUsing the `glimpse()` function, we see that the dataset consists of 30,000 rows and 32 columns. The output displays the column names, their data types, and the first few entries for each variable. Additionally, there are 15 extra columns (Extra_Column_1 to Extra_Column_15) which are not clearly defined.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(heart_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 30,000\nColumns: 32\n$ Age                     <dbl> 56, 69, 46, 32, 60, 25, 78, 38, 56, 75, 36, 40…\n$ Gender                  <chr> \"Male\", \"Male\", \"Male\", \"Female\", \"Female\", \"F…\n$ Region                  <chr> \"Urban\", \"Urban\", \"Rural\", \"Urban\", \"Rural\", \"…\n$ Smoking_History         <chr> \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"Y…\n$ Diabetes_History        <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No…\n$ Hypertension_History    <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No…\n$ Cholesterol_Level       <dbl> 186.4002, 185.1367, 210.6966, 211.1655, 223.81…\n$ Physical_Activity       <chr> \"Moderate\", \"Low\", \"Low\", \"Moderate\", \"High\", …\n$ Diet_Quality            <chr> \"Poor\", \"Good\", \"Average\", \"Good\", \"Good\", \"Go…\n$ Alcohol_Consumption     <chr> \"Low\", \"Low\", \"Moderate\", \"High\", \"High\", \"Hig…\n$ Stress_Levels           <dbl> 3.644786, 3.384056, 3.810911, 6.014878, 6.8068…\n$ BMI                     <dbl> 33.96135, 28.24287, 27.60121, 23.71729, 19.771…\n$ Heart_Rate              <dbl> 72.30153, 57.45764, 64.65870, 55.13147, 76.667…\n$ Systolic_BP             <dbl> 123.90209, 129.89331, 145.65490, 131.78522, 10…\n$ Diastolic_BP            <dbl> 85.68281, 73.52426, 71.99481, 68.21133, 92.902…\n$ Family_History          <chr> \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No…\n$ Heart_Attack_Occurrence <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ Extra_Column_1          <dbl> 0.40498852, 0.03627815, 0.85297888, 0.39085280…\n$ Extra_Column_2          <dbl> 0.43330004, 0.51256694, 0.21959083, 0.29684675…\n$ Extra_Column_3          <dbl> 0.62871236, 0.66839275, 0.61343656, 0.15572404…\n$ Extra_Column_4          <dbl> 0.70160955, 0.11552874, 0.50800995, 0.87025144…\n$ Extra_Column_5          <dbl> 0.49814235, 0.42381938, 0.90066981, 0.39035591…\n$ Extra_Column_6          <dbl> 0.007901312, 0.083932768, 0.227205241, 0.40318…\n$ Extra_Column_7          <dbl> 0.79458257, 0.68895108, 0.49634358, 0.74140891…\n$ Extra_Column_8          <dbl> 0.29077922, 0.83016364, 0.75210679, 0.22396813…\n$ Extra_Column_9          <dbl> 0.49719307, 0.63449028, 0.18150125, 0.32931387…\n$ Extra_Column_10         <dbl> 0.52199452, 0.30204337, 0.62918031, 0.14319054…\n$ Extra_Column_11         <dbl> 0.79965663, 0.04368285, 0.01827617, 0.90778075…\n$ Extra_Column_12         <dbl> 0.72239788, 0.45166789, 0.06322702, 0.54232201…\n$ Extra_Column_13         <dbl> 0.1487387, 0.8786714, 0.1465122, 0.9224606, 0.…\n$ Extra_Column_14         <dbl> 0.8340099, 0.5356022, 0.9972962, 0.6262165, 0.…\n$ Extra_Column_15         <dbl> 0.061632229, 0.617825340, 0.974455410, 0.22860…\n```\n\n\n:::\n:::\n\n\n\nThe following provides an overview of the Japan Heart Attack dataset using the `ExpData()` function, summarizing both overall and variable-level details. \n\n::: panel-tabset\n### Overall data summary\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary1 <- heart_data %>%\n  ExpData(type = 1)\n\n# Display the summary (further customization possible)\nsummary1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                                          Descriptions     Value\n1                                   Sample size (nrow)     30000\n2                              No. of variables (ncol)        32\n3                    No. of numeric/interger variables        22\n4                              No. of factor variables         0\n5                                No. of text variables        10\n6                             No. of logical variables         0\n7                          No. of identifier variables        20\n8                                No. of date variables         0\n9             No. of zero variance variables (uniform)         0\n10               %. of variables having complete cases 100% (32)\n11   %. of variables having >0% and <50% missing cases    0% (0)\n12 %. of variables having >=50% and <90% missing cases    0% (0)\n13          %. of variables having >=90% missing cases    0% (0)\n```\n\n\n:::\n:::\n\n\n\n### Variable level summary\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary2 <- heart_data %>%\n  ExpData(type = 2)\n\n# Display the summary (further customization possible)\nsummary2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Index           Variable_Name Variable_Type Sample_n Missing_Count\n1      1                     Age       numeric    30000             0\n2      2                  Gender     character    30000             0\n3      3                  Region     character    30000             0\n4      4         Smoking_History     character    30000             0\n5      5        Diabetes_History     character    30000             0\n6      6    Hypertension_History     character    30000             0\n7      7       Cholesterol_Level       numeric    30000             0\n8      8       Physical_Activity     character    30000             0\n9      9            Diet_Quality     character    30000             0\n10    10     Alcohol_Consumption     character    30000             0\n11    11           Stress_Levels       numeric    30000             0\n12    12                     BMI       numeric    30000             0\n13    13              Heart_Rate       numeric    30000             0\n14    14             Systolic_BP       numeric    30000             0\n15    15            Diastolic_BP       numeric    30000             0\n16    16          Family_History     character    30000             0\n17    17 Heart_Attack_Occurrence     character    30000             0\n18    18          Extra_Column_1       numeric    30000             0\n19    19          Extra_Column_2       numeric    30000             0\n20    20          Extra_Column_3       numeric    30000             0\n21    21          Extra_Column_4       numeric    30000             0\n22    22          Extra_Column_5       numeric    30000             0\n23    23          Extra_Column_6       numeric    30000             0\n24    24          Extra_Column_7       numeric    30000             0\n25    25          Extra_Column_8       numeric    30000             0\n26    26          Extra_Column_9       numeric    30000             0\n27    27         Extra_Column_10       numeric    30000             0\n28    28         Extra_Column_11       numeric    30000             0\n29    29         Extra_Column_12       numeric    30000             0\n30    30         Extra_Column_13       numeric    30000             0\n31    31         Extra_Column_14       numeric    30000             0\n32    32         Extra_Column_15       numeric    30000             0\n   Per_of_Missing No_of_distinct_values\n1               0                    62\n2               0                     2\n3               0                     2\n4               0                     2\n5               0                     2\n6               0                     2\n7               0                 30000\n8               0                     3\n9               0                     3\n10              0                     4\n11              0                 29613\n12              0                 30000\n13              0                 30000\n14              0                 30000\n15              0                 30000\n16              0                     2\n17              0                     2\n18              0                 30000\n19              0                 30000\n20              0                 30000\n21              0                 30000\n22              0                 30000\n23              0                 30000\n24              0                 30000\n25              0                 30000\n26              0                 30000\n27              0                 30000\n28              0                 30000\n29              0                 30000\n30              0                 30000\n31              0                 30000\n32              0                 30000\n```\n\n\n:::\n:::\n\n\n\n:::\n\n### Convert categorical variables to factors\n\nFrom the overview above, we see that the dataset contains no missing values, and the categorical variables have a maximum of 4 unique values. Converting these variables into factors ensures they are correctly treated as categorical data during analysis and visualization.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert selected categorical variables into factors\nheart_data <- heart_data %>%\n  mutate(\n    Gender = as.factor(Gender),\n    Region = as.factor(Region),\n    Smoking_History = as.factor(Smoking_History),\n    Diabetes_History = as.factor(Diabetes_History),\n    Hypertension_History = as.factor(Hypertension_History),\n    Physical_Activity = as.factor(Physical_Activity),\n    Diet_Quality = as.factor(Diet_Quality),\n    Alcohol_Consumption = as.factor(Alcohol_Consumption),\n    Family_History = as.factor(Family_History),\n    Heart_Attack_Occurrence = as.factor(Heart_Attack_Occurrence)\n  )\n```\n:::\n\n\n\n### Drop extra columns\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Select only the Extra_Columns and the outcome variable\nextra_data <- heart_data %>%\n  select(starts_with(\"Extra_Column_\"), Heart_Attack_Occurrence)\n\n# Reshape to long format\nextra_data_long <- melt(extra_data, id.vars = \"Heart_Attack_Occurrence\")\n\n# Create boxplots comparing each Extra_Column by Heart_Attack_Occurrence\nggplot(extra_data_long, aes(x = Heart_Attack_Occurrence, y = value)) +\n  geom_boxplot() +\n  facet_wrap(~ variable, scales = \"free\") +\n  labs(\n    title = \"Distribution of Extra Columns by Heart Attack Occurrence\",\n    x = \"Heart Attack Occurrence\",\n    y = \"Value\"\n  )\n```\n\n::: {.cell-output-display}\n![](Take-home_Ex1_files/figure-html/unnamed-chunk-7-1.png){width=1152}\n:::\n:::\n\n\nSince these variables do not appear to vary by heart attack status, they are unlikely to provide useful information for any downstream analysis (e.g., modeling, hypothesis testing). Dropping them will simplify the dataset and help focus on variables that do relate to heart attack risk. \n\nWe can drop them with the following code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nheart_data <- heart_data %>%\n  select(-starts_with(\"Extra_Column_\"))\n```\n:::\n\n\n\n\n### Cleaned dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(heart_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 30,000\nColumns: 17\n$ Age                     <dbl> 56, 69, 46, 32, 60, 25, 78, 38, 56, 75, 36, 40…\n$ Gender                  <fct> Male, Male, Male, Female, Female, Female, Male…\n$ Region                  <fct> Urban, Urban, Rural, Urban, Rural, Rural, Urba…\n$ Smoking_History         <fct> Yes, No, Yes, No, No, No, No, Yes, No, No, No,…\n$ Diabetes_History        <fct> No, No, No, No, No, No, Yes, No, No, No, No, N…\n$ Hypertension_History    <fct> No, No, No, No, No, No, Yes, No, Yes, No, Yes,…\n$ Cholesterol_Level       <dbl> 186.4002, 185.1367, 210.6966, 211.1655, 223.81…\n$ Physical_Activity       <fct> Moderate, Low, Low, Moderate, High, Low, High,…\n$ Diet_Quality            <fct> Poor, Good, Average, Good, Good, Good, Poor, P…\n$ Alcohol_Consumption     <fct> Low, Low, Moderate, High, High, High, High, No…\n$ Stress_Levels           <dbl> 3.644786, 3.384056, 3.810911, 6.014878, 6.8068…\n$ BMI                     <dbl> 33.96135, 28.24287, 27.60121, 23.71729, 19.771…\n$ Heart_Rate              <dbl> 72.30153, 57.45764, 64.65870, 55.13147, 76.667…\n$ Systolic_BP             <dbl> 123.90209, 129.89331, 145.65490, 131.78522, 10…\n$ Diastolic_BP            <dbl> 85.68281, 73.52426, 71.99481, 68.21133, 92.902…\n$ Family_History          <fct> No, Yes, No, No, No, No, No, No, No, Yes, Yes,…\n$ Heart_Attack_Occurrence <fct> No, No, No, No, No, No, No, No, Yes, No, No, N…\n```\n\n\n:::\n:::\n\n\n## Exploratory visuals \n\n### Create new variables\n\nWe create a new variable, `Age_Group`, classifying individuals as “Over50” or “50OrBelow” to compare younger vs. older individuals.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nheart_data_eda <- heart_data %>%\n  mutate(Age_Group = ifelse(Age > 50, \"Over50\", \"50OrBelow\") %>% as.factor())\n```\n:::\n\n\n\n\nWe create `AgeGender` by combining the `Age_Group` and `gender`. We also combine `smoking status` and `physical activity` into `SmokeAct` and reorder `alcohol consumption` levels.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Demographic variables\nheart_data_eda <- heart_data_eda %>%\n  mutate(\n    AgeGender = case_when(\n      Age_Group == \"Over50\" & Gender == \"Male\"   ~ \"Over 50 Male\",\n      Age_Group == \"Over50\" & Gender == \"Female\" ~ \"Over 50 Female\",\n      Age_Group == \"50OrBelow\" & Gender == \"Male\"   ~ \"≤50 Male\",\n      Age_Group == \"50OrBelow\" & Gender == \"Female\" ~ \"≤50 Female\"\n    ) %>% factor(levels = c(\"≤50 Female\",\"≤50 Male\",\"Over 50 Female\",\"Over 50 Male\"))\n  )\n\n# Lifestyle variables\nheart_data_eda <- heart_data_eda %>%\n  mutate(\n    SmokeAct = case_when(\n      Smoking_History == \"Yes\" & Physical_Activity == \"Low\"      ~ \"Smoker, PA:Low\",\n      Smoking_History == \"Yes\" & Physical_Activity == \"Moderate\" ~ \"Smoker, PA:Mod\",\n      Smoking_History == \"Yes\" & Physical_Activity == \"High\"     ~ \"Smoker, PA:High\",\n      Smoking_History == \"No\"  & Physical_Activity == \"Low\"      ~ \"Non-Smoker, PA:Low\",\n      Smoking_History == \"No\"  & Physical_Activity == \"Moderate\" ~ \"Non-Smoker, PA:Mod\",\n      Smoking_History == \"No\"  & Physical_Activity == \"High\"     ~ \"Non-Smoker, PA:High\"\n    ) %>% \n    # Order them in a sensible sequence:\n    factor(levels = c(\"Non-Smoker, PA:Low\",\"Non-Smoker, PA:Mod\",\"Non-Smoker, PA:High\",\n                      \"Smoker, PA:Low\",\"Smoker, PA:Mod\",\"Smoker, PA:High\"))\n  )\n```\n:::\n\n\n\n\n### Mosaic Plot: Demographic Analysis\nWe plot a mosaic where `AgeGender` is on the x-axis, color indicates heart attack occurrence, and each facet represents a different region.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_demo <- ggplot(heart_data_eda) +\n  geom_mosaic(\n    aes(x = product(AgeGender),\n        fill = Heart_Attack_Occurrence,\n        text = paste0(\"Group: \", AgeGender,\n                      \"<br>Region: \", Region,\n                      \"<br>Heart Attack: \", Heart_Attack_Occurrence)\n    ),\n    alpha = 0.9\n  ) +\n  facet_wrap(~ Region) +\n  scale_fill_manual(values = c(\"No\" = \"#F1B1B5\", \"Yes\" = \"#97B3AE\")) +\n  labs(\n    title = \"Demographic Mosaic: Age & Gender by Region vs. Heart Attack\",\n    x     = \"Age & Gender\",\n    y     = \" \",\n    fill  = \"Heart Attack\"\n  ) +\n  theme_minimal()\n\np_demo\n```\n\n::: {.cell-output-display}\n![](Take-home_Ex1_files/figure-html/unnamed-chunk-12-1.png){width=1152}\n:::\n:::\n\n\n##### Explanation of the plot\n\nThis mosaic plot illustrates heart attack occurrences across different age and gender groups within rural and urban regions. The width of each bar segment corresponds to the relative size of that demographic group, while the height indicates the proportion of individuals who experienced a heart attack. \n\nOverall, heart attack rates remain relatively consistent between rural and urban areas. However, males tend to have a higher probability of heart attack than females, regardless of age or region.\n\n\n### Mosaic plot: Lifestyle factors\n\nWe create a mosaic plot with `SmokeAct` on the x-axis, color by heart attack occurrence, and facet by the four alcohol consumption levels.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Reorder factor levels for Alcohol_Consumption\nheart_data_eda <- heart_data_eda %>%\n  mutate(\n    Alcohol_Consumption = factor(\n      Alcohol_Consumption,\n      levels = c(\"High\", \"Moderate\", \"Low\", \"None\")\n    )\n  )\n\nggplot(heart_data_eda) +\n  geom_mosaic(aes(\n    x    = product(SmokeAct),\n    fill = Heart_Attack_Occurrence\n  ), alpha = 0.9) +\n  facet_wrap(~ Alcohol_Consumption, ncol = 2) +\n  scale_fill_manual(values = c(\"No\" = \"#F1B1B5\", \"Yes\" = \"#97B3AE\")) +\n  labs(\n    title = \"Lifestyle Mosaic: Smoking, Activity, and Alcohol vs. Heart Attack\",\n    subtitle = \"PA = Physical Activity. Each facet represents a different Alcohol Consumption level.\",\n    x = \"Smoking & PA Group\",\n    y = \"\",\n    fill = \"Heart Attack\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title    = element_text(face = \"bold\", size = 14, hjust=0.5),\n    plot.subtitle = element_text(size = 10, hjust=0.5),\n    strip.text    = element_text(face=\"bold\"),\n    axis.text.x   = element_text(angle=40, hjust=1, size=7),\n    panel.spacing = unit(2, \"lines\")\n  )\n```\n\n::: {.cell-output-display}\n![](Take-home_Ex1_files/figure-html/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n##### Explanation of the plot\n\n\nThis mosaic plot explores how smoking, physical activity (PA), and alcohol consumption interact to influence heart attack occurrences. Each facet represents a different alcohol consumption level (High, Moderate, Low, None).\n\nInterestingly, non-smokers who report no alcohol consumption but high physical activity exhibit one of the highest heart attack rates. Additionally, smokers with moderate physical activity tend to have higher heart attack rates compared to smokers with low or high physical activity.\n\n### Pairwise numeric plot (Health metrics)\nThis code uses `ggpairs()` to create a matrix of pairwise plots for all numeric variables in heart_data. The `mapping = aes(color = Heart_Attack_Occurrence)` argument adds a color-coded grouping by heart attack status.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Automatically select all numeric columns from the dataset\nnumeric_cols <- sapply(heart_data, is.numeric)\n\npairwise_plot <- ggpairs(\n  data = heart_data,\n  columns = which(numeric_cols),\n  mapping = aes(color = Heart_Attack_Occurrence),\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.3, size = 0.5)),\n  diag = list(continuous = wrap(\"densityDiag\", alpha = 0.5)),\n  upper = list(continuous = wrap(\"cor\", size = 4))\n) +\n  ggtitle(\"Pairwise Correlations Among All Numeric Metrics\")\n\npairwise_plot\n```\n\n::: {.cell-output-display}\n![](Take-home_Ex1_files/figure-html/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n\n##### Explanation of the plot\n\nThis grid compares health metrics like BMI, blood pressure, cholesterol, and stress. The **diagonal panels** show density curves for each variable, revealing, for instance, that `Age` has a broader distribution compared to the other variables.\n\nThe **upper panels** list correlation coefficients and their significance, most of which are near zero (e.g., **`Corr: 0.025, 0.048`**), indicating that these variables do not strongly co-vary. In the **lower scatter plots**, points are colored by heart attack occurrence; no tight clustering suggests no single numeric threshold exclusively separates “Yes” vs. “No.” For instance, `Systolic_BP` and `Diastolic_BP` show little correlation as high `Systolic_BP` often coexists with both high and low `Diastolic_BP`. Overall, **no single numeric factor** stands out as a strictly linear driver of heart attack, though there may be subtle nonlinear or interactive effects to explore later.\n\n\n## Train test split\n\nBefore building a predictive model, it is best practice to split the data into training and testing sets. The `createDataPartition` function ensures that the distribution of the target class is approximately the same in both sets. Here, we allocate 80% of the data for training and 20% for testing.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\ntrain_index <- createDataPartition(heart_data$Heart_Attack_Occurrence, p = 0.8, list = FALSE)\n\ntrain_data <- heart_data[train_index, ]\ntest_data  <- heart_data[-train_index, ]\n```\n:::\n\n\n\n\n## Naive logistic regression\n\nHere, we build an initial (“naive”) logistic regression model that includes **all** available predictors (except the 15 “Extra_Column” variables we dropped). This approach gives us a baseline.\n\n\n### Fit the model\n\nWe will fit a logistic regression using `glm()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Use a standard glm with all predictors\nnaive_glm <- glm(\n  Heart_Attack_Occurrence ~ .,\n  data   = train_data,\n  family = binomial\n)\n```\n:::\n\n\n\n\n### Understanding the model\n\nWe use `check_collinearity()` to see if any variables are highly correlated or cause near‐complete separation. A “good” logistic regression typically avoids extremely high VIFs or indefinite confidence intervals.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Capture the output\nresult <- check_collinearity(naive_glm)\n\n# Coerce to a data frame\ndf <- as.data.frame(result)\n\n# Use knitr::kable to print the table neatly\nknitr::kable(df, caption = \"Check for Multicollinearity\", \n             format = \"html\", \n             table.attr = \"style='width:100%; white-space:nowrap;'\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style='width:100%; white-space:nowrap;'>\n<caption>Check for Multicollinearity</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Term </th>\n   <th style=\"text-align:right;\"> VIF </th>\n   <th style=\"text-align:right;\"> VIF_CI_low </th>\n   <th style=\"text-align:right;\"> VIF_CI_high </th>\n   <th style=\"text-align:right;\"> SE_factor </th>\n   <th style=\"text-align:right;\"> Tolerance </th>\n   <th style=\"text-align:right;\"> Tolerance_CI_low </th>\n   <th style=\"text-align:right;\"> Tolerance_CI_high </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Age </td>\n   <td style=\"text-align:right;\"> 1.001325 </td>\n   <td style=\"text-align:right;\"> 1.000000 </td>\n   <td style=\"text-align:right;\"> 1.991615e+01 </td>\n   <td style=\"text-align:right;\"> 1.000662 </td>\n   <td style=\"text-align:right;\"> 0.9986771 </td>\n   <td style=\"text-align:right;\"> 0.0502105 </td>\n   <td style=\"text-align:right;\"> 0.9999999 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Gender </td>\n   <td style=\"text-align:right;\"> 1.000943 </td>\n   <td style=\"text-align:right;\"> 1.000000 </td>\n   <td style=\"text-align:right;\"> 6.378053e+02 </td>\n   <td style=\"text-align:right;\"> 1.000472 </td>\n   <td style=\"text-align:right;\"> 0.9990575 </td>\n   <td style=\"text-align:right;\"> 0.0015679 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Region </td>\n   <td style=\"text-align:right;\"> 1.000670 </td>\n   <td style=\"text-align:right;\"> 1.000000 </td>\n   <td style=\"text-align:right;\"> 1.062715e+05 </td>\n   <td style=\"text-align:right;\"> 1.000335 </td>\n   <td style=\"text-align:right;\"> 0.9993302 </td>\n   <td style=\"text-align:right;\"> 0.0000094 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Smoking_History </td>\n   <td style=\"text-align:right;\"> 1.000446 </td>\n   <td style=\"text-align:right;\"> 1.000000 </td>\n   <td style=\"text-align:right;\"> 9.589080e+08 </td>\n   <td style=\"text-align:right;\"> 1.000223 </td>\n   <td style=\"text-align:right;\"> 0.9995547 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Diabetes_History </td>\n   <td style=\"text-align:right;\"> 1.001228 </td>\n   <td style=\"text-align:right;\"> 1.000000 </td>\n   <td style=\"text-align:right;\"> 3.808479e+01 </td>\n   <td style=\"text-align:right;\"> 1.000614 </td>\n   <td style=\"text-align:right;\"> 0.9987733 </td>\n   <td style=\"text-align:right;\"> 0.0262572 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Hypertension_History </td>\n   <td style=\"text-align:right;\"> 1.000834 </td>\n   <td style=\"text-align:right;\"> 1.000000 </td>\n   <td style=\"text-align:right;\"> 3.250209e+03 </td>\n   <td style=\"text-align:right;\"> 1.000417 </td>\n   <td style=\"text-align:right;\"> 0.9991664 </td>\n   <td style=\"text-align:right;\"> 0.0003077 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Cholesterol_Level </td>\n   <td style=\"text-align:right;\"> 1.000587 </td>\n   <td style=\"text-align:right;\"> 1.000000 </td>\n   <td style=\"text-align:right;\"> 1.351909e+06 </td>\n   <td style=\"text-align:right;\"> 1.000293 </td>\n   <td style=\"text-align:right;\"> 0.9994134 </td>\n   <td style=\"text-align:right;\"> 0.0000007 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Physical_Activity </td>\n   <td style=\"text-align:right;\"> 1.001852 </td>\n   <td style=\"text-align:right;\"> 1.000002 </td>\n   <td style=\"text-align:right;\"> 2.745436e+00 </td>\n   <td style=\"text-align:right;\"> 1.000926 </td>\n   <td style=\"text-align:right;\"> 0.9981510 </td>\n   <td style=\"text-align:right;\"> 0.3642408 </td>\n   <td style=\"text-align:right;\"> 0.9999980 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Diet_Quality </td>\n   <td style=\"text-align:right;\"> 1.001445 </td>\n   <td style=\"text-align:right;\"> 1.000000 </td>\n   <td style=\"text-align:right;\"> 1.030279e+01 </td>\n   <td style=\"text-align:right;\"> 1.000722 </td>\n   <td style=\"text-align:right;\"> 0.9985567 </td>\n   <td style=\"text-align:right;\"> 0.0970611 </td>\n   <td style=\"text-align:right;\"> 0.9999998 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Alcohol_Consumption </td>\n   <td style=\"text-align:right;\"> 1.002116 </td>\n   <td style=\"text-align:right;\"> 1.000005 </td>\n   <td style=\"text-align:right;\"> 1.852209e+00 </td>\n   <td style=\"text-align:right;\"> 1.001057 </td>\n   <td style=\"text-align:right;\"> 0.9978885 </td>\n   <td style=\"text-align:right;\"> 0.5398958 </td>\n   <td style=\"text-align:right;\"> 0.9999947 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Stress_Levels </td>\n   <td style=\"text-align:right;\"> 1.000919 </td>\n   <td style=\"text-align:right;\"> 1.000000 </td>\n   <td style=\"text-align:right;\"> 8.893873e+02 </td>\n   <td style=\"text-align:right;\"> 1.000459 </td>\n   <td style=\"text-align:right;\"> 0.9990821 </td>\n   <td style=\"text-align:right;\"> 0.0011244 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> BMI </td>\n   <td style=\"text-align:right;\"> 1.000849 </td>\n   <td style=\"text-align:right;\"> 1.000000 </td>\n   <td style=\"text-align:right;\"> 2.561245e+03 </td>\n   <td style=\"text-align:right;\"> 1.000424 </td>\n   <td style=\"text-align:right;\"> 0.9991522 </td>\n   <td style=\"text-align:right;\"> 0.0003904 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Heart_Rate </td>\n   <td style=\"text-align:right;\"> 1.000834 </td>\n   <td style=\"text-align:right;\"> 1.000000 </td>\n   <td style=\"text-align:right;\"> 3.252876e+03 </td>\n   <td style=\"text-align:right;\"> 1.000417 </td>\n   <td style=\"text-align:right;\"> 0.9991665 </td>\n   <td style=\"text-align:right;\"> 0.0003074 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Systolic_BP </td>\n   <td style=\"text-align:right;\"> 1.000939 </td>\n   <td style=\"text-align:right;\"> 1.000000 </td>\n   <td style=\"text-align:right;\"> 6.753992e+02 </td>\n   <td style=\"text-align:right;\"> 1.000469 </td>\n   <td style=\"text-align:right;\"> 0.9990618 </td>\n   <td style=\"text-align:right;\"> 0.0014806 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Diastolic_BP </td>\n   <td style=\"text-align:right;\"> 1.000920 </td>\n   <td style=\"text-align:right;\"> 1.000000 </td>\n   <td style=\"text-align:right;\"> 8.726101e+02 </td>\n   <td style=\"text-align:right;\"> 1.000460 </td>\n   <td style=\"text-align:right;\"> 0.9990807 </td>\n   <td style=\"text-align:right;\"> 0.0011460 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Family_History </td>\n   <td style=\"text-align:right;\"> 1.000826 </td>\n   <td style=\"text-align:right;\"> 1.000000 </td>\n   <td style=\"text-align:right;\"> 3.716482e+03 </td>\n   <td style=\"text-align:right;\"> 1.000413 </td>\n   <td style=\"text-align:right;\"> 0.9991743 </td>\n   <td style=\"text-align:right;\"> 0.0002691 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n#### Interpreting the collinearity results\n\n- VIF ~1.0 but extremely large upper confidence bounds: This indicates the algorithm is unsure about the exact magnitude of possible collinearity. In simpler terms, the model’s variance–covariance matrix is nearly singular.\n- This often happens when:\n  - Quasi‐complete separation: Certain variables or combinations nearly “perfectly” predict the outcome.\n  - Imbalance in the dataset (many more “No” than “Yes”) plus insufficient signal in some predictors.\n  - Over‐parametrization: Too many correlated predictors for the sample size.\n\n### Model performance\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1) Collinearity plot\ncheck_c <- check_collinearity(naive_glm)\np_collinearity <- plot(check_c) +\n  theme(axis.text.x = element_text(angle = 40, hjust = 1))\n\n# 2) Confusion matrix heatmap\npred_prob_naive <- predict(naive_glm, newdata = test_data, type = \"response\")\n\npred_class_naive <- ifelse(pred_prob_naive >= 0.5, \"Yes\", \"No\") %>%\n  factor(levels = levels(test_data$Heart_Attack_Occurrence))\n\n# Evaluate\nnaive_results <- data.frame(\n  obs   = test_data$Heart_Attack_Occurrence,\n  pred  = pred_class_naive,\n  prob  = pred_prob_naive\n)\n\nnaive_cm <- naive_results %>%\n  conf_mat(obs, pred)\n\np_confmat <- autoplot(naive_cm, type = \"heatmap\") +\n  labs(title = \"Naive Logistic Regression: Confusion Matrix\")\n\n# 3) ROC curve as a ggplot object using ggroc()\nroc_naive <- roc(\n  response  = as.numeric(naive_results$obs),\n  predictor = as.numeric(naive_results$prob)\n)\n\np_roc <- ggroc(roc_naive, colour = \"#1c61b6\", legacy.axes = TRUE) +\n  labs(title = \"ROC Curve: Naïve Logistic Model\") +\n  theme_minimal()\n\n\ncombined_plot <- p_collinearity / (p_confmat + p_roc)\n\n# Display the combined plot\ncombined_plot\n```\n\n::: {.cell-output-display}\n![](Take-home_Ex1_files/figure-html/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n\n#### Explanation of the plot\n\n1. **Collinearity Plot:**  \n  - This bar chart displays VIF estimates, where the ideal values fall in the green region.\n  - Individual VIF point estimates hover around 1.0, but their upper confidence intervals extend into the red, indicating extremely high values. \n  - This suggests that the model’s parameter estimates are unstable due to quasi‐complete separation or an excess of correlated predictors relative to the sample size. In essence, the model cannot reliably discern each variable’s true contribution, leading to artificially low VIF point estimates paired with massive uncertainty bounds.\n\n2. **Confusion Matrix:**  \n  - The matrix shows 0 true positives. \n  - This is typical when a logistic model either encounters near-complete separation or opts to disregard the minority class in imbalanced datasets.\n\n3. **ROC Curve:**  \n  - The ROC curve lies near the diagonal reference line, confirming that the model lacks predictive power and is essentially guessing.\n\n\n## Improving logistic regression\n\n### Rationale\n\nOur naïve logistic regression suggested potential issues:\n- Very low sensitivity (predicting all “No”)\n- Large variance inflation factor (VIF) intervals\n\nTherefore, we refine the logistic model by:\n\n1. Use **weighted** logistic regression to handle class imbalance,  \n2. Incorporate mild non-linear terms for `BMI`, `Systolic_BP`, and `Diastolic_BP` with polynomial expansions,  \n3. Remove redundant variables.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_train <- train_data\nnew_test  <- test_data\n\n\nnew_formula <- as.formula(\n  \"Heart_Attack_Occurrence ~ Age + Gender + Family_History + poly(BMI, 2, raw=TRUE) + \n   Heart_Rate + poly(Systolic_BP, 2, raw=TRUE) + poly(Diastolic_BP, 2, raw=TRUE) + Cholesterol_Level + \n   Diabetes_History + Hypertension_History + Physical_Activity + Smoking_History + \n   Diet_Quality + Alcohol_Consumption + Stress_Levels\"\n)\n```\n:::\n\n\n\n\n#### Creating observation weights\nWe create balanced weights to give more importance to the minority class. This ensures misclassifying a minority‐class “Yes” is penalized more strongly than misclassifying a “No.”\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_yes <- sum(new_train$Heart_Attack_Occurrence == \"Yes\")\nn_no  <- sum(new_train$Heart_Attack_Occurrence == \"No\")\nN     <- n_yes + n_no\n\nw_yes <- N / (2 * n_yes)\nw_no  <- N / (2 * n_no)\n\n# Assign weights in the new training dataset\nnew_train$weights_col <- ifelse(\n  new_train$Heart_Attack_Occurrence == \"Yes\",\n  w_yes,\n  w_no\n)\n```\n:::\n\n\n\n\n### Fit weighted logistic model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_glm_weighted <- glm(\n  formula = new_formula,\n  data    = new_train,\n  family  = binomial(link = \"logit\"),\n  weights = weights_col\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Capture the output\nresult <- check_collinearity(model_glm_weighted)\n\n# Coerce to a data frame\ndf <- as.data.frame(result)\n\n# Use knitr::kable to print the table neatly\nknitr::kable(df, caption = \"Check for Multicollinearity\", \n             format = \"html\", \n             table.attr = \"style='width:100%; white-space:nowrap;'\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style='width:100%; white-space:nowrap;'>\n<caption>Check for Multicollinearity</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Term </th>\n   <th style=\"text-align:right;\"> VIF </th>\n   <th style=\"text-align:right;\"> VIF_CI_low </th>\n   <th style=\"text-align:right;\"> VIF_CI_high </th>\n   <th style=\"text-align:right;\"> SE_factor </th>\n   <th style=\"text-align:right;\"> Tolerance </th>\n   <th style=\"text-align:right;\"> Tolerance_CI_low </th>\n   <th style=\"text-align:right;\"> Tolerance_CI_high </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Age </td>\n   <td style=\"text-align:right;\"> 1.004556 </td>\n   <td style=\"text-align:right;\"> 1.000277 </td>\n   <td style=\"text-align:right;\"> 1.074857 </td>\n   <td style=\"text-align:right;\"> 1.002275 </td>\n   <td style=\"text-align:right;\"> 0.9954646 </td>\n   <td style=\"text-align:right;\"> 0.9303566 </td>\n   <td style=\"text-align:right;\"> 0.9997228 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Gender </td>\n   <td style=\"text-align:right;\"> 1.003347 </td>\n   <td style=\"text-align:right;\"> 1.000075 </td>\n   <td style=\"text-align:right;\"> 1.149809 </td>\n   <td style=\"text-align:right;\"> 1.001672 </td>\n   <td style=\"text-align:right;\"> 0.9966644 </td>\n   <td style=\"text-align:right;\"> 0.8697096 </td>\n   <td style=\"text-align:right;\"> 0.9999252 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Family_History </td>\n   <td style=\"text-align:right;\"> 1.001847 </td>\n   <td style=\"text-align:right;\"> 1.000002 </td>\n   <td style=\"text-align:right;\"> 2.774072 </td>\n   <td style=\"text-align:right;\"> 1.000923 </td>\n   <td style=\"text-align:right;\"> 0.9981564 </td>\n   <td style=\"text-align:right;\"> 0.3604809 </td>\n   <td style=\"text-align:right;\"> 0.9999981 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> poly(BMI, 2, raw = TRUE) </td>\n   <td style=\"text-align:right;\"> 1.003982 </td>\n   <td style=\"text-align:right;\"> 1.000162 </td>\n   <td style=\"text-align:right;\"> 1.097600 </td>\n   <td style=\"text-align:right;\"> 1.001989 </td>\n   <td style=\"text-align:right;\"> 0.9960340 </td>\n   <td style=\"text-align:right;\"> 0.9110785 </td>\n   <td style=\"text-align:right;\"> 0.9998376 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Heart_Rate </td>\n   <td style=\"text-align:right;\"> 1.002266 </td>\n   <td style=\"text-align:right;\"> 1.000008 </td>\n   <td style=\"text-align:right;\"> 1.614256 </td>\n   <td style=\"text-align:right;\"> 1.001132 </td>\n   <td style=\"text-align:right;\"> 0.9977391 </td>\n   <td style=\"text-align:right;\"> 0.6194804 </td>\n   <td style=\"text-align:right;\"> 0.9999916 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> poly(Systolic_BP, 2, raw = TRUE) </td>\n   <td style=\"text-align:right;\"> 1.005107 </td>\n   <td style=\"text-align:right;\"> 1.000419 </td>\n   <td style=\"text-align:right;\"> 1.062202 </td>\n   <td style=\"text-align:right;\"> 1.002550 </td>\n   <td style=\"text-align:right;\"> 0.9949186 </td>\n   <td style=\"text-align:right;\"> 0.9414402 </td>\n   <td style=\"text-align:right;\"> 0.9995808 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> poly(Diastolic_BP, 2, raw = TRUE) </td>\n   <td style=\"text-align:right;\"> 1.004244 </td>\n   <td style=\"text-align:right;\"> 1.000211 </td>\n   <td style=\"text-align:right;\"> 1.085491 </td>\n   <td style=\"text-align:right;\"> 1.002120 </td>\n   <td style=\"text-align:right;\"> 0.9957736 </td>\n   <td style=\"text-align:right;\"> 0.9212419 </td>\n   <td style=\"text-align:right;\"> 0.9997893 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Cholesterol_Level </td>\n   <td style=\"text-align:right;\"> 1.001808 </td>\n   <td style=\"text-align:right;\"> 1.000002 </td>\n   <td style=\"text-align:right;\"> 3.013365 </td>\n   <td style=\"text-align:right;\"> 1.000904 </td>\n   <td style=\"text-align:right;\"> 0.9981953 </td>\n   <td style=\"text-align:right;\"> 0.3318549 </td>\n   <td style=\"text-align:right;\"> 0.9999984 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Diabetes_History </td>\n   <td style=\"text-align:right;\"> 1.002857 </td>\n   <td style=\"text-align:right;\"> 1.000033 </td>\n   <td style=\"text-align:right;\"> 1.244352 </td>\n   <td style=\"text-align:right;\"> 1.001427 </td>\n   <td style=\"text-align:right;\"> 0.9971512 </td>\n   <td style=\"text-align:right;\"> 0.8036315 </td>\n   <td style=\"text-align:right;\"> 0.9999666 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Hypertension_History </td>\n   <td style=\"text-align:right;\"> 1.002289 </td>\n   <td style=\"text-align:right;\"> 1.000009 </td>\n   <td style=\"text-align:right;\"> 1.586196 </td>\n   <td style=\"text-align:right;\"> 1.001144 </td>\n   <td style=\"text-align:right;\"> 0.9977158 </td>\n   <td style=\"text-align:right;\"> 0.6304391 </td>\n   <td style=\"text-align:right;\"> 0.9999911 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Physical_Activity </td>\n   <td style=\"text-align:right;\"> 1.004614 </td>\n   <td style=\"text-align:right;\"> 1.000291 </td>\n   <td style=\"text-align:right;\"> 1.073216 </td>\n   <td style=\"text-align:right;\"> 1.002304 </td>\n   <td style=\"text-align:right;\"> 0.9954073 </td>\n   <td style=\"text-align:right;\"> 0.9317786 </td>\n   <td style=\"text-align:right;\"> 0.9997093 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Smoking_History </td>\n   <td style=\"text-align:right;\"> 1.002518 </td>\n   <td style=\"text-align:right;\"> 1.000016 </td>\n   <td style=\"text-align:right;\"> 1.390398 </td>\n   <td style=\"text-align:right;\"> 1.001258 </td>\n   <td style=\"text-align:right;\"> 0.9974880 </td>\n   <td style=\"text-align:right;\"> 0.7192187 </td>\n   <td style=\"text-align:right;\"> 0.9999838 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Diet_Quality </td>\n   <td style=\"text-align:right;\"> 1.003581 </td>\n   <td style=\"text-align:right;\"> 1.000102 </td>\n   <td style=\"text-align:right;\"> 1.125179 </td>\n   <td style=\"text-align:right;\"> 1.001789 </td>\n   <td style=\"text-align:right;\"> 0.9964314 </td>\n   <td style=\"text-align:right;\"> 0.8887471 </td>\n   <td style=\"text-align:right;\"> 0.9998975 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Alcohol_Consumption </td>\n   <td style=\"text-align:right;\"> 1.007005 </td>\n   <td style=\"text-align:right;\"> 1.001124 </td>\n   <td style=\"text-align:right;\"> 1.043643 </td>\n   <td style=\"text-align:right;\"> 1.003496 </td>\n   <td style=\"text-align:right;\"> 0.9930440 </td>\n   <td style=\"text-align:right;\"> 0.9581821 </td>\n   <td style=\"text-align:right;\"> 0.9988770 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Stress_Levels </td>\n   <td style=\"text-align:right;\"> 1.002948 </td>\n   <td style=\"text-align:right;\"> 1.000039 </td>\n   <td style=\"text-align:right;\"> 1.219987 </td>\n   <td style=\"text-align:right;\"> 1.001473 </td>\n   <td style=\"text-align:right;\"> 0.9970609 </td>\n   <td style=\"text-align:right;\"> 0.8196808 </td>\n   <td style=\"text-align:right;\"> 0.9999605 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n### Visualizing the model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_prob_improved <- predict(model_glm_weighted, newdata = test_data, type = \"response\")\n\n\nggplot(mapping = aes(x = pred_prob_improved)) +\n  geom_histogram(bins = 30, fill = \"skyblue\", color = \"white\") +\n  labs(title = \"Distribution of Predicted Probabilities (Naïve Logistic)\",\n       x = \"Predicted Probability of Heart Attack\",\n       y = \"Count\")\n```\n\n::: {.cell-output-display}\n![](Take-home_Ex1_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\nFrom the histogram above, we can see that the model is overpredicting positive cases at `0.50` threshold. As the positive cases is approximately 10% of the dataset, we will take a threshold of `0.55` instead.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_c <- check_collinearity(model_glm_weighted)\np_collinearity <- plot(check_c) +\n  labs(title = \"Collinearity of Weighted Logistic Model\") +\n  theme(axis.text.x = element_text(angle = 40, hjust = 1))\n\npred_prob_improved <- predict(model_glm_weighted, newdata = test_data, type = \"response\")\npred_class_improved <- ifelse(pred_prob_improved >= 0.55, \"Yes\", \"No\") %>%\n  factor(levels = levels(test_data$Heart_Attack_Occurrence))\nimproved_cm <- data.frame(\n  obs  = test_data$Heart_Attack_Occurrence,\n  pred = pred_class_improved\n) %>% conf_mat(obs, pred)\n\np_confmat <- autoplot(improved_cm, type = \"heatmap\") +\n  labs(title = \"Weighted Logistic: Confusion Matrix\")\n\nroc_improved <- roc(\n  response  = as.numeric(test_data$Heart_Attack_Occurrence),\n  predictor = as.numeric(pred_prob_improved)\n)\np_roc <- ggroc(roc_improved, colour = \"#1c61b6\", legacy.axes = TRUE) +\n  labs(title = \"ROC Curve: Weighted Logistic Model\") +\n  theme_minimal()\n\n## Combine the three plots\nlibrary(patchwork)\ncombined_plot <- p_collinearity / (p_confmat + p_roc)\ncombined_plot\n```\n\n::: {.cell-output-display}\n![](Take-home_Ex1_files/figure-html/unnamed-chunk-24-1.png){width=1152}\n:::\n:::\n\n\n\n\n#### Weighted Logistic Regression Results\n\nAfter applying class weights and mild polynomial terms to BMI and Blood Pressure, our weighted logistic model shows some improvements compared to the naïve model:\n\n- Sensitivity Improves: The model now predicts a small number of “Yes” cases rather than labeling everything “No.”\n\n- Collinearity appears more stable\n  - The VIF plot still shows point estimates around 1.0 for most variables, with moderate spikes in confidence intervals for a few. However, these are less extreme than in the naïve model, suggesting the parameter estimates are more stable overall.\n\n- Little change to overall accuracy (AUC)\n  - While the model does somewhat better at identifying positives, the ROC curve remains fairly close to the diagonal, reflecting an AUC only slightly better than 0.5.\n  - In other words, the model is still not very accurate overall, indicating that the available predictors may not strongly discriminate between “Yes” and “No”—or that we need further refinements (e.g., more complex interactions, alternative transformations, or additional data).\n\n\n## Fitting a xgboost\n\n### Data preparation\n\nWe first convert the outcome `Heart_Attack_Occurrence` to a 0/1 numeric variable. Then, we build model matrices using `model.matrix()` which transforms both categorical and numeric predictors into a suitable format for XGBoost.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert outcome to 0/1\ntrain_data_xgb <- train_data %>%\n  mutate(YesNo = ifelse(Heart_Attack_Occurrence == \"Yes\", 1, 0))\n\nx_train <- model.matrix(YesNo ~ . - Heart_Attack_Occurrence, data=train_data_xgb)\ny_train <- train_data_xgb$YesNo\n\ntest_data_xgb <- test_data %>%\n  mutate(YesNo = ifelse(Heart_Attack_Occurrence == \"Yes\", 1, 0))\nx_test <- model.matrix(YesNo ~ . - Heart_Attack_Occurrence, data=test_data_xgb)\ny_test <- test_data_xgb$YesNo\n\nn_yes <- sum(y_train == 1)\nn_no  <- sum(y_train == 0)\nscale_pos <- n_no / n_yes\n```\n:::\n\n\n\n\n### XGBoost model training\n\nWe create DMatrix objects for both training and test sets, then specify key hyperparameters like `max_depth`, `eta`, and `scale_pos_weight.` The model is trained with early stopping if the test AUC does not improve after a certain number of rounds.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndtrain <- xgb.DMatrix(data = x_train, label = y_train)\ndtest  <- xgb.DMatrix(data = x_test,  label = y_test)\n\nparam <- list(\n  objective        = \"binary:logistic\",\n  eval_metric      = \"auc\",             # can also track \"error\" or \"logloss\"\n  max_depth        = 10,\n  eta              = 0.2,\n  scale_pos_weight = scale_pos          # imbalance correction\n)\n\n# Train with 100 rounds\nset.seed(123)\nxgb_model <- xgb.train(\n  params   = param,\n  data     = dtrain,\n  nrounds  = 1000,\n  watchlist= list(train=dtrain, test=dtest),\n  early_stopping_rounds = 50,  # optional, for early stop\n  print_every_n          = 10\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]\ttrain-auc:0.703267\ttest-auc:0.493556 \nMultiple eval metrics are present. Will use test_auc for early stopping.\nWill train until test_auc hasn't improved in 50 rounds.\n\n[11]\ttrain-auc:0.893593\ttest-auc:0.519498 \n[21]\ttrain-auc:0.955144\ttest-auc:0.513634 \n[31]\ttrain-auc:0.977267\ttest-auc:0.519925 \n[41]\ttrain-auc:0.985509\ttest-auc:0.514492 \n[51]\ttrain-auc:0.993414\ttest-auc:0.505698 \n[61]\ttrain-auc:0.996438\ttest-auc:0.503955 \n[71]\ttrain-auc:0.998506\ttest-auc:0.502356 \nStopping. Best iteration:\n[28]\ttrain-auc:0.972693\ttest-auc:0.525233\n```\n\n\n:::\n:::\n\n\nTraining AUC may reach near 1.0 (overfitting), but test AUC remains around 0.52, implying the model struggles to find a robust pattern. However, this may also indicate a difference in distribution of the training and testing dataset, and that no robust pattern is learnable from these features. There may also be insufficient predictive signal in the data.\n\n\n### Model evaluation\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_prob_xgb <- predict(xgb_model, newdata = dtest, iteration_range = xgb_model$best_iteration)\n\npred_class_xgb <- ifelse(pred_prob_xgb >= 0.5, 1, 0)\n\nresults_xgb <- data.frame(\n  obs  = factor(y_test, levels=c(0,1), labels=c(\"No\",\"Yes\")),\n  pred = factor(pred_class_xgb, levels=c(0,1), labels=c(\"No\",\"Yes\")),\n  prob = pred_prob_xgb\n)\n\nxgb_cm <- conf_mat(results_xgb, truth=obs, estimate=pred)\nxgb_cm %>% summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   <chr>                <chr>          <dbl>\n 1 accuracy             binary        0.763 \n 2 kap                  binary        0.0124\n 3 sens                 binary        0.826 \n 4 spec                 binary        0.191 \n 5 ppv                  binary        0.903 \n 6 npv                  binary        0.107 \n 7 mcc                  binary        0.0131\n 8 j_index              binary        0.0167\n 9 bal_accuracy         binary        0.508 \n10 detection_prevalence binary        0.824 \n11 precision            binary        0.903 \n12 recall               binary        0.826 \n13 f_meas               binary        0.863 \n```\n\n\n:::\n:::\n\n\n\n### ROC curve and confusion matrix\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_confmat <- autoplot(xgb_cm, type=\"heatmap\") +\n  labs(title=\"XGBoost Confusion Matrix\", fill=\"Count\")\n\nroc_xgb <- roc(response = as.numeric(results_xgb$obs), predictor = results_xgb$prob)\nauc_val <- auc(roc_xgb)\n\np_roc <- ggroc(roc_xgb, colour=\"#1c61b6\") +\n  labs(\n    title = paste0(\"XGBoost ROC Curve (AUC=\", round(auc_val,3), \")\"),\n    x     = \"1 - Specificity\",\n    y     = \"Sensitivity\"\n  ) +\n  theme_minimal()\n\n\n\ncombined_plot <- p_confmat | p_roc\n\ncombined_plot\n```\n\n::: {.cell-output-display}\n![](Take-home_Ex1_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\n#### Plot explanation:\n\n- Confusion matrix: Shows how many “No” vs. “Yes” cases are classified correctly vs. incorrectly. Despite the class‐imbalance correction, the model still misclassifies most “Yes” events.\n- ROC curve: The curve hovers close to the diagonal, with an AUC near ~0.52. This is only marginally better than random guessing (AUC=0.5).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimportance_xgb <- xgb.importance(model = xgb_model)\nimportance_xgb  # see a data frame of feature importances\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                        Feature        Gain       Cover   Frequency\n                         <char>       <num>       <num>       <num>\n 1:           Cholesterol_Level 0.145242292 0.155522476 0.136697155\n 2:                 Systolic_BP 0.136727355 0.167277235 0.132375113\n 3:               Stress_Levels 0.134981157 0.185245402 0.134184340\n 4:                Diastolic_BP 0.134287572 0.160294404 0.126846919\n 5:                         BMI 0.131566444 0.126584570 0.123932053\n 6:                  Heart_Rate 0.123786314 0.121115391 0.121921801\n 7:                         Age 0.083851778 0.050432098 0.092371093\n 8:           Family_HistoryYes 0.010253500 0.002744379 0.011056388\n 9:   Physical_ActivityModerate 0.009907179 0.004210412 0.011659463\n10:                 RegionUrban 0.009613210 0.001768051 0.012262539\n11:          Smoking_HistoryYes 0.009402740 0.003311462 0.011357925\n12:     Hypertension_HistoryYes 0.009347006 0.001676643 0.010754850\n13:                  GenderMale 0.009129164 0.005765353 0.011357925\n14:         Diabetes_HistoryYes 0.008797618 0.001136887 0.009850236\n15:      Alcohol_ConsumptionLow 0.008275298 0.001408138 0.010553825\n16:            Diet_QualityGood 0.007770305 0.001091494 0.009146648\n17: Alcohol_ConsumptionModerate 0.007592931 0.001353723 0.009850236\n18:            Diet_QualityPoor 0.007177992 0.003669527 0.007940497\n19:     Alcohol_ConsumptionNone 0.006439186 0.004718671 0.007638959\n20:        Physical_ActivityLow 0.005850960 0.000673683 0.008242034\n                        Feature        Gain       Cover   Frequency\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot\nxgb.plot.importance(importance_xgb, top_n = 15, \n                    main=\"XGBoost Feature Importance\")\n```\n\n::: {.cell-output-display}\n![](Take-home_Ex1_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n\n#### Interpretation:\n\nWhile certain features (e.g., `Cholesterol_Level`, `Systolic_BP`) rank highest in splitting power, the test AUC is still low. This suggests either insufficient predictive signal or potential overfitting to training noise.\n\n\n## Final thoughts\nAcross multiple approaches (naive logistic, weighted logistic, XGBoost), the models struggle to achieve predictive accuracy on the test set. This may indicate that the available features (after dropping the undefined extras) do not strongly distinguish between heart attack occurrences. Additional data, refined feature engineering, or domain expertise might be necessary to improve predictive performance. Nonetheless, the exploratory visualizations provide insights into demographic and lifestyle patterns associated with heart attack risk in Japan.",
    "supporting": [
      "Take-home_Ex1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}